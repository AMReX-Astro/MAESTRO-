

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Managing Jobs &mdash; MAESTROeX 19.04.1
 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"Macros": {"rhozero": "{\\rho_0}", "pizero": "{\\pi_0}", "pizeroone": "{\\pi_0^{(1)}}", "pizerotwo": "{\\pi_0^{(2)}}", "gammabar": "{\\overline{\\Gamma}_1}", "ptl": "{\\partial}", "eb": "{{\\bf e}}", "fb": "{{\\bf f}}", "ib": "{{\\bf i}}", "Ub": "{{\\bf U}}", "Vb": "{{\\bf V}}", "xb": "{{\\bf x}}", "ut": "{{\\tilde{u}}}", "vt": "{{\\tilde{v}}}", "wt": "{{\\tilde{w}}}", "Ubt": "{\\widetilde{\\Ub}}", "edge": "{{\\rm EDGE}}", "mac": "{{\\rm MAC}}", "trans": "{{\\rm TRANS}}", "nablab": "{\\mathbf{\\nabla}}", "cdotb": "{\\mathbf{\\cdot}}", "half": "{\\frac{1}{2}}", "myhalf": "{\\frac{1}{2}}", "nph": "{{n + \\myhalf}}", "nmh": "{{n - \\myhalf}}", "Hext": "{{H_{\\rm ext}}}", "Hnuc": "{{H_{\\rm nuc}}}", "kth": "{k_{\\rm th}}", "pred": "{{\\rm pred}}", "Sbar": "{\\overline{S}}", "inp": "{\\mathrm{in}}", "initp": "{\\mathrm{init}}", "outp": "{\\mathrm{out}}", "uadv": "{\\widetilde{\\mathbf{U}}^{\\mathrm{ADV}}}", "uadvone": "{\\widetilde{\\mathbf{U}}^{\\mathrm{ADV},\\star}}", "uadvonedag": "{\\Ubt^{\\mathrm{ADV},\\dagger,\\star}}", "uadvtwo": "{\\widetilde{\\mathbf{U}}^{\\mathrm{ADV}}}", "uadvtwodag": "{\\Ubt^{\\mathrm{ADV},\\dagger}}", "uadvsdcstar": "{\\mathbf{U}^{\\mathrm{ADV},\\star}}", "uadvsdcpred": "{\\mathbf{U}^{\\mathrm{ADV},\\mathrm{pred}}}", "uadvsdc": "{\\mathbf{U}^{\\mathrm{ADV}}}", "dt": "{\\Delta t}", "dr": "{\\Delta r}", "etarho": "{\\eta_{\\rho}}", "etarhoec": "{\\etarho^{\\rm ec}}", "etarhocc": "{\\etarho^{\\rm cc}}", "etarhoflux": "{\\etarho^{\\rm flux}}", "divetarho": "{\\nabla\\cdot(\\etarho\\eb_r)}", "ow": "{\\overline{w}_0}", "dw": "{\\delta w_0}", "thalf": "{\\frac{3}{2}}", "rhop": "{{\\rho^{\\prime}}}", "omegadot": "{\\dot{\\omega}}", "er": "{\\mathbf{e}_r}", "ex": "{\\mathbf{e}_x}", "ey": "{\\mathbf{e}_y}", "ez": "{\\mathbf{e}_z}", "Omegab": "{{\\bf \\Omega}}", "rb": "{{\\bf r}}", "boxtype": "{{\\tt box\\/}}", "fab": "{{\\tt fab\\/}}", "multifab": "{{\\tt multifab\\/}}", "boxarray": "{{\\tt boxarray\\/}}", "mlboxarray": "{{\\tt ml\\_boxarray\\/}}", "layout": "{{\\tt layout\\/}}", "mllayout": "{{\\tt ml\\_layout\\/}}", "bctower": "{{\\tt bc\\_tower\\/}}", "bclevel": "{{\\tt bc\\_level\\/}}", "mgtower": "{{\\tt mg\\_tower\\/}}", "bndryreg": "{{\\tt bndry\\_reg\\/}}", "runparam": ["{\\index{Runtime parameters!{\\tt #1}}{\\tt #1}}", 1], "runparamidx": ["{\\index{Runtime parameters!{\\tt #1}}}", 1], "code": ["{\\index{Code reference!{\\tt #1}}{\\tt #1}}", 1], "codeidx": ["{\\index{Code reference!{\\tt #1}}}", 1], "otherindex": ["{\\index{#1!#2}}", 2], "fdamp": "{{f_\\mathrm{damp}}}"}}})</script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/theme_overrides.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Frequently Asked Questions" href="faq.html" />
    <link rel="prev" title="MAESTROeX Build System" href="makefiles.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> MAESTROeX
          

          
          </a>

          
            
            
              <div class="version">
                19.04.1

              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">MAESTROeX basics</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="preface.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction to MAESTROeX</a></li>
<li class="toctree-l1"><a class="reference internal" href="flowchart.html">MAESTROeX Flowchart</a></li>
</ul>
<p class="caption"><span class="caption-text">Using MAESTROeX</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="problems.html">Problem Setups</a></li>
<li class="toctree-l1"><a class="reference internal" href="unit_tests.html">Unit Tests</a></li>
<li class="toctree-l1"><a class="reference internal" href="common_parameters.html">Common Runtime Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">MAESTROeX Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="param_intro.html">Runtime Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="initial_models.html">Initial Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="visualization.html">Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="analysis.html">Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="makefiles.html">MAESTROeX Build System</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Managing Jobs</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#general-info">General Info</a></li>
<li class="toctree-l2"><a class="reference internal" href="#linux-boxes">Linux boxes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#gfortran">gfortran</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pgi-compilers">PGI compilers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#working-at-olcf-ornl">Working at OLCF (ORNL)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#titan-compilers">Titan Compilers</a></li>
<li class="toctree-l3"><a class="reference internal" href="#monitoring-allocations">Monitoring Allocations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#automatic-restarting-and-archiving-of-data">Automatic Restarting and Archiving of Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#profiling-and-debugging-on-gpus">Profiling and Debugging on GPUs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#score-p-with-cube-and-vampir">Score-P with CUBE and vampir</a></li>
<li class="toctree-l4"><a class="reference internal" href="#nvprof-and-nvvp">nvprof and nvvp</a></li>
<li class="toctree-l4"><a class="reference internal" href="#target-metrics">Target Metrics</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#batch-submission-of-yt-visualization-scripts">Batch Submission of yt Visualization Scripts</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#rheapreferred-method">Rhea—preferred method</a></li>
<li class="toctree-l4"><a class="reference internal" href="#titan">Titan</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#remote-visit-visualization-on-lens">Remote VisIt Visualization on Lens</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#working-at-nersc">Working at NERSC</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#edison-compilers">edison compilers</a></li>
<li class="toctree-l3"><a class="reference internal" href="#running-jobs">Running Jobs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#automatic-restarting-and-archiving-of-data-1">Automatic Restarting and Archiving of Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#batch-visualization-using-yt">Batch visualization using yt</a></li>
<li class="toctree-l3"><a class="reference internal" href="#using-the-amrpostprocesing-python-plotting-scripts-on-hopper">Using the AmrPostprocesing python plotting scripts on hopper</a></li>
<li class="toctree-l3"><a class="reference internal" href="#remote-visualization-on-hopper">Remote visualization on hopper</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#working-at-ncsa-blue-waters">Working at NCSA (Blue Waters)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#bw-compilers">BW Compilers</a></li>
<li class="toctree-l3"><a class="reference internal" href="#monitoring-allocations-1">Monitoring Allocations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">Frequently Asked Questions</a></li>
</ul>
<p class="caption"><span class="caption-text">MAESTROeX technical details</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="lo_density.html">Low Density Cutoffs</a></li>
<li class="toctree-l1"><a class="reference internal" href="volume_discrepancy.html">Volume Discrepancy Factor</a></li>
<li class="toctree-l1"><a class="reference internal" href="eos_notes.html">Equation of State Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="networks.html">Reaction Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="eta.html">The Mixing Term, <span class="math notranslate nohighlight">\(\etarho\)</span></a></li>
<li class="toctree-l1"><a class="reference internal" href="eta.html#eta-flow-chart"><span class="math notranslate nohighlight">\(\eta\)</span> Flow Chart</a></li>
<li class="toctree-l1"><a class="reference internal" href="eta.html#computing-etarhoec-and-etarhocc">Computing <span class="math notranslate nohighlight">\(\etarhoec\)</span> and <span class="math notranslate nohighlight">\(\etarhocc\)</span></a></li>
<li class="toctree-l1"><a class="reference internal" href="eta.html#using-etarhoec">Using <span class="math notranslate nohighlight">\(\etarhoec\)</span></a></li>
<li class="toctree-l1"><a class="reference internal" href="eta.html#using-etarhocc">Using <span class="math notranslate nohighlight">\(\etarhocc\)</span></a></li>
<li class="toctree-l1"><a class="reference internal" href="pert.html">Interface State Details</a></li>
<li class="toctree-l1"><a class="reference internal" href="sdc.html">Spectral Deferred Corrections</a></li>
<li class="toctree-l1"><a class="reference internal" href="Godunov.html">Godunov Interface States</a></li>
<li class="toctree-l1"><a class="reference internal" href="mg.html">Multigrid</a></li>
<li class="toctree-l1"><a class="reference internal" href="rotation.html">Rotation in MAESTROeX</a></li>
<li class="toctree-l1"><a class="reference internal" href="spherical_basestate.html">Modifications for a Spherical Self-Gravitating Star</a></li>
<li class="toctree-l1"><a class="reference internal" href="planar_invsq_basestate.html">Modifications for a <span class="math notranslate nohighlight">\(1/r^2\)</span> Plane-Parallel Basestate</a></li>
<li class="toctree-l1"><a class="reference internal" href="thermo_notes.html">Notes on Thermodynamics</a></li>
<li class="toctree-l1"><a class="reference internal" href="beta0.html">Notes on <span class="math notranslate nohighlight">\(\beta_0\)</span></a></li>
<li class="toctree-l1"><a class="reference internal" href="enthalpy.html">Notes on Enthalpy</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MAESTROeX</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Managing Jobs</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/managingjobs.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="managing-jobs">
<h1>Managing Jobs<a class="headerlink" href="#managing-jobs" title="Permalink to this headline">¶</a></h1>
<div class="section" id="general-info">
<h2>General Info<a class="headerlink" href="#general-info" title="Permalink to this headline">¶</a></h2>
<p>All plotfile directories have a job_info file which lists as
host of parameters about the simulation, including:</p>
<ul class="simple">
<li><p>A descriptive name for the simulation (the job_name runtime
parameter</p></li>
<li><p>The number of MPI tasks and OpenMP threads</p></li>
<li><p>The total amount of CPU-hours used up to this point</p></li>
<li><p>The data and time of the plotfile creation and the directory it was written to.</p></li>
<li><p>The build date, machine, and directory</p></li>
<li><p>The MAESTRO, AMReX, and other relevant git hashes for the source</p></li>
<li><p>The directories used in the build</p></li>
<li><p>The compilers used and compilation flags</p></li>
<li><p>The number of levels and boxes for the grid</p></li>
<li><p>The properties of the species carried</p></li>
<li><p>The tolerances for the MG solves</p></li>
<li><p>Any warnings from the initialization procedure (note: these are not currently stored on restart).</p></li>
<li><p>The value of all runtime parameters (even those that were not explicitly set in the inputs file), along with an indicator showing if the default value
was overridden.</p></li>
</ul>
<p>This file makes it easy to understand how to recreate the run that
produced the plotfile.</p>
</div>
<div class="section" id="linux-boxes">
<h2>Linux boxes<a class="headerlink" href="#linux-boxes" title="Permalink to this headline">¶</a></h2>
<div class="section" id="gfortran">
<h3>gfortran<a class="headerlink" href="#gfortran" title="Permalink to this headline">¶</a></h3>
<p>gfortran is probably the best-supported compiler for MAESTRO. Here are some
version-specific notes:</p>
<ul>
<li><p><em>gfortran 4.8.x</em>: This typically works well, but sometimes we get
an error allocating memory in cluster_f.f90. This
is a compiler bug (affecting atleast 4.8.2 and 4.8.3):</p>
<p>The code runs without any problem if it is compiled with -O2
-ftree-vectorize -fno-range-check (our default) but with
cluster_f.f90 compiled with -O2 -ftree-vectorize
-fno-range-check -fno-tree-pre. The “fno-tree-pre” option
turns off “ftree-pre” that is turned on by “O2”</p>
<p>GCC manual says,</p>
<blockquote>
<div><p>-ftree-pre
Perform partial redundancy elimination (PRE) on trees. This flag is enabled by default at -O2 and -O3.</p>
</div></blockquote>
<p>gfortran 4.8.5 appears to work without issues</p>
</li>
<li><p><em>gfortran 5.1.1</em>: These compilers have no known issues.</p></li>
<li><p><em>gfortran 5.3.x</em>: These compilers have no known issues.</p></li>
<li><p><em>gfortran 6.2</em>: These compilers work without any known issues.</p>
<p>gfortran 6.2.1 is used for nightly regression testing.</p>
</li>
</ul>
</div>
<div class="section" id="pgi-compilers">
<h3>PGI compilers<a class="headerlink" href="#pgi-compilers" title="Permalink to this headline">¶</a></h3>
<p>The AMReX floating point exception trapping is disabled with PGI
compilers earlier than version 16, due to problems with PGI using the
system header files. From version 16 onward, things should work.</p>
<p>There are no known issues with PGI 16.5 compilers—these are used
for nightly regression testing.</p>
</div>
</div>
<div class="section" id="working-at-olcf-ornl">
<h2>Working at OLCF (ORNL)<a class="headerlink" href="#working-at-olcf-ornl" title="Permalink to this headline">¶</a></h2>
<div class="section" id="titan-compilers">
<h3>Titan Compilers<a class="headerlink" href="#titan-compilers" title="Permalink to this headline">¶</a></h3>
<p>The preferred compilers on Titan are the Cray compilers.
Cray 8.4.0 works well on titan/OLCF with MPI/OpenMP.</p>
</div>
<div class="section" id="monitoring-allocations">
<h3>Monitoring Allocations<a class="headerlink" href="#monitoring-allocations" title="Permalink to this headline">¶</a></h3>
<p>The showusage and showusage -f commands give an
overview of the usage.</p>
</div>
<div class="section" id="automatic-restarting-and-archiving-of-data">
<h3>Automatic Restarting and Archiving of Data<a class="headerlink" href="#automatic-restarting-and-archiving-of-data" title="Permalink to this headline">¶</a></h3>
<p>The submission script titan.run and shell script
process.titan in Util/job_scripts/titan/
are designed to allow you to run MAESTRO with minimal interaction,
while being assured that the data is archived to HPSS on the OLCF
machines.</p>
<p>To use the scripts, first create a directory in HPSS that has the same
name as the directory on lustre you are running in (just the directory
name, not the full path). E.g. if you are running in a directory
call wdconvect_run, then do:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hsi</span>
<span class="n">mkdir</span> <span class="n">wdconvect_run</span>
</pre></div>
</div>
<p>(Note: if the hsi command prompts you for your password, you will need
to talk to the OLCF help desk to ask for password-less access to
HPSS).</p>
<p>The script process.titan is called from titan.run and will
run in the background and continually wait until checkpoint or
plotfiles are created (actually, it always leaves the most recent one
alone, since data may still be written to it, so it waits until there
are more than 1 in the directory).</p>
<p>Then the script will use htar to archive the plotfiles and
checkpoints to HPSS. If the htar command was successful, then
the plotfiles are copied into a plotfile/ subdirectory. This is
actually important, since you don’t want to try archiving the data a
second time and overwriting the stored copy, especially if a purge
took place.</p>
<p>Additionally, if the ftime executable is in your path (
ftime.f90 lives in amrex/Tools/Postprocessing/F_src/), then
the script will create a file called ftime.out that lists the
name of the plotfile and the corresponding simulation time.</p>
<p>Finally, right when the job is submitted, the script will tar up all
of the diagnostic files created by diag.f90 and ftime.out
and archive them on HPSS. The .tar file is given a name that
contains the date-string to allow multiple archives to co-exist.</p>
<p>The titan.run submission script has code in it that will look at
the most recently generated checkpoint files, make sure that they were
written out correctly (it looks to see if there is a Header file,
since that is the last thing written), and automatically set the
–restart flag on the run line to restart from the most recent
checkpoint file. This allows you to job-chain a bunch of submission
and have them wait until the previous job finished and then
automatically queue up:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">qsub</span> <span class="o">-</span><span class="n">W</span> <span class="n">depend</span><span class="o">=</span><span class="n">afterany</span><span class="p">:</span><span class="o">&lt;</span><span class="n">JOB</span><span class="o">-</span><span class="n">ID</span><span class="o">&gt;</span>  <span class="o">&lt;</span><span class="n">QSUB</span> <span class="n">SCRIPT</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>where is the id number of the job that must complete
before the new submission runs and QSUB SCRIPT is the submission
script (e.g. titan.run). This way you can queue up a bunch of
runs and literally leave things alone and it will restart from the
right place automatically and store the data as it is generated.</p>
<p>When process.titan is running, it creates a lockfile (called
process.pid) that ensures that only one instance of the script
is running at any one time. Sometimes if the machine crashes, the
process.pid file will be left behind, in which case, the script
aborts. Just delete that if you know the script is not running.</p>
<p>The chainsub.sh script can be used to automatically launch a
number of jobs depending on a single, currently queued (or running)
job.</p>
</div>
<div class="section" id="profiling-and-debugging-on-gpus">
<h3>Profiling and Debugging on GPUs<a class="headerlink" href="#profiling-and-debugging-on-gpus" title="Permalink to this headline">¶</a></h3>
<p>To get an idea of how code performs on Titan’s GPUs, there are a few tools
available. We’ll overview a few here.</p>
<div class="section" id="score-p-with-cube-and-vampir">
<h4>Score-P with CUBE and vampir<a class="headerlink" href="#score-p-with-cube-and-vampir" title="Permalink to this headline">¶</a></h4>
<p>Score-P is a profiling and tracing tool that can be used to instrument
code to generate data for other tools to analyze, such as CUBE and
vampir. These tools have been developed to analyze performance of HPC
codes that run on several nodes, not specifically for analyzing GPU usage.
Still, they do support some GPU analysis. In the next section, we’ll discuss
NVIDA’s tools specifically for analyzing GPU usage.
At the time of writing, Score-P usage is fairly well documented on OLCF’s
website here: <a class="reference external" href="https://www.olcf.ornl.gov/kb_articles/software-scorep/">https://www.olcf.ornl.gov/kb_articles/software-scorep/</a>.
We’ll review the essentials here, but please see the link for more details.</p>
<p>To instrument a code with Score-P you must re-compile. First, your
desired modules will need to be loaded. Please note that <em>order is
important</em> — you need to load modules needed for compilation before loading
Score-P. The Score-P module is designed to detect the loaded
environment and will configure itself based on that. These tools have been
tested with the PGI 16.3.0 compilers, and we will use them in our examples.
One possible set of module loads is</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ module load PrgEnv-pgi
$ module swap pgi/15.7.0 pgi/16.3.0
$ module load cudatoolkit
$ module load scorep/3.0-rc1
</pre></div>
</div>
<p>In the above we’ve loaded version 3.0, release candidate 1, which added some
support for analyzing OpenACC code. The next step is to compile. You
essentially preface your normal compile (and link) line with the Score-P
executable and options. As an example using the Fortran wrapper required on
Titan, we have</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ scorep --cuda --openacc -v ftn gpuprogram.f90
</pre></div>
</div>
<p>One way to achieve this in MAESTRO is to modify the appropriate make file. For
PGI, this would be $AMREX_HOME/Tools/F_mk/comps/Linux_pgi.mak. If
this proves useful, it may be worth it to build Score-P into the build
infrastructure.</p>
<p>Once compiled, we are ready to generate profiling and tracing data. Among those
that develop these tools, note that they draw a distinction between profiling
and tracing. Profiling generates a timing (or perhaps other metric) summary of
the entire program’s execution while tracing produces a timeline of the
execution. Score-P’s analysis is configured with environment variables.
Some of the key configuration variables used in testing include</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">SCOREP_ENABLE_PROFILING</span><span class="o">=</span>yes
<span class="nb">export</span> <span class="nv">SCOREP_ENABLE_TRACING</span><span class="o">=</span>yes
<span class="nb">export</span> <span class="nv">SCOREP_EXPERIMENT_DIRECTORY</span><span class="o">=</span>profile-trace
<span class="nb">export</span> <span class="nv">SCOREP_CUDA_ENABLE</span><span class="o">=</span>yes,kernel_counter,flushatexit
<span class="nb">export</span> <span class="nv">SCOREP_CUDA_BUFFER</span><span class="o">=</span>200M
<span class="nb">export</span> <span class="nv">SCOREP_TOTAL_MEMORY</span><span class="o">=</span>1G
<span class="nb">export</span> <span class="nv">SCOREP_OPENACC_ENABLE</span><span class="o">=</span>yes
</pre></div>
</div>
<p>For a full listing and definition of configuration variables, execute</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ scorep-info config-vars --full
</pre></div>
</div>
<p>Except for very simple codes, you will never want to enable both tracing and
profiling. The overhead is too high, and the code will likely crash or be
excessively slow. Typically, it’s best to profile first and then trace. The
profiling data can be used to help configure tracing (as we’ll see shortly).</p>
<p>Once the configuration is set, simply run the code as you normally would.
Experience suggests you will need to load the same modules that were loaded for
compilation when executing. If analysis is being done through a batch script,
note that you cannot do a simple module load … in the script. First you
need to do source /opt/modules/default/init/bash in the script, and then
module loads will work as usual.</p>
<p>After executing, analysis data will be stored in the specified
SCOREP_EXPERIMENT_DIRECTORY. With profiling, you
will see a file like profile.cubex. This can be opened with cube
(module load cube).</p>
<p>As mentioned, the profiling data can be used to get recommended settings for
tracing. Running</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ scorep-score -r profile.cubex
</pre></div>
</div>
<p>will yield output showing estimated sizes for e.g. SCOREP_TOTAL_MEMORY.
It also list functions that are called many times. If you don’t care about
them and they’re slowing Score-P down (or making an outrageously large output
file), you can configure Score-P to ignore them in its analysis. To filter a set
of functions, you need to provide a filter file, for example</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ <span class="nb">export</span> <span class="nv">SCOREP_FILTERING_FILE</span><span class="o">=</span>scorep.filter
</pre></div>
</div>
<p>where</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ cat scorep.filter
SCOREP_REGION_NAMES_BEGIN
 EXCLUDE
   matmul_sub
   matvec_sub
SCOREP_REGION_NAMES_END
</pre></div>
</div>
<p>This would tell Score-P not to trace the routines matmul_sub and
matvec_sub. See the OLCF KnowledgeBase article and/or Score-P’s
help for more, but this doesn’t seem to be the best-documented aspect of the
program.</p>
<p>Running with tracing enabled will generate a traces.otf2 file that can be
inspected with vampir (module load vampir)</p>
</div>
<div class="section" id="nvprof-and-nvvp">
<h4>nvprof and nvvp<a class="headerlink" href="#nvprof-and-nvvp" title="Permalink to this headline">¶</a></h4>
<p>NVIDIA provides tools for specifically analyzing how your code utilizes their
GPUs. Score-P is a fully-featured profiler with some CUDA and OpenACC
support. It can be useful for providing context for GPU execution and it allows
you to, for example, see line numbers for OpenACC directives that are executed.
nvprof will only analyze GPU execution, but in exchange you get much more
detail than is available with Score-P. nvvp is NVIDIA’s visual
profiler. It can be used to read data generated by nvprof. Most useful
is the guided analysis it will perform, which analyzes your code’s GPU
performance for bottlenecks and suggests ways to improve performance. Both are
provided when you load the cudatoolkit module.</p>
<p>With nvprof, no instrumentation is necessary. Instead, you compile
normally and then run nvprof on the executable. As before, be sure when
executing to load the modules used at compile-time. Executing nvprof on
Titan’s compute nodes requires some unexpected options having to do with how
aprun and nvprof interact.</p>
<p>To get a basic overview printed to the terminal on Titan’s compute node, execute</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ aprun -b nvprof --profile-child-processes ./gpuprogram.exe arg1 arg2...
</pre></div>
</div>
<p>To generate tracing data for nvvp, execute</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ aprun -b nvprof --profile-child-processes -o nvprof.timeline.out%p
  ./gpuprogram.exe arg1 arg2...
</pre></div>
</div>
<p>nvvp can then be used to read nvprof.timeline.out%p, where the
%p will be replaced with the process ID. You <em>must</em> include %p in
the output file’s name or the code will crash, even if you’re not running a
multi-process code.</p>
<p>To generate profile-like metric data for nvvp, execute</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ aprun -b nvprof --profile-child-processes --analysis-metrics
  -o nvprof.metrics.out%p ./gpuprogram.exe arg1 arg2...
</pre></div>
</div>
<p>This is the output needed for nvvp’s guided analysis.</p>
</div>
<div class="section" id="target-metrics">
<h4>Target Metrics<a class="headerlink" href="#target-metrics" title="Permalink to this headline">¶</a></h4>
<p>The output from profilers may be difficult to makes sense of. The purpose of
this section is to note different metrics and reasonable targets for them.
Note that these may be specific to the k20x hardware in Titan.</p>
<ul class="simple">
<li><p>Threads per block: 256-512. Note that if your code requires many
registers per thread, then this will limit the number of threads per block.</p></li>
<li><p>Occupancy: 60% is a reasonable target. We have had success with codes
even achieving only 23% occupancy.</p></li>
</ul>
<p>One very useful tool for determining target metrics and what is limiting your
performance is a spreadsheet developed by NVIDIA to calculate occupancy. Every
installation of the CUDA Toolkit should have this occupancy calculator in a
tools subdirectory. At time of writing, the calculator is also available at
this link:
<a class="reference external" href="http://developer.download.nvidia.com/compute/cuda/CUDA_Occupancy_calculator.xls">http://developer.download.nvidia.com/compute/cuda/CUDA_Occupancy_calculator.xls</a>.
The document is actually more than a simple calculator. It contains quite a
bit of interesting insight into optimizing a GPU code. More on occupancy can be
found here:
<a class="reference external" href="http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy">http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy</a>.</p>
</div>
</div>
<div class="section" id="batch-submission-of-yt-visualization-scripts">
<h3>Batch Submission of yt Visualization Scripts<a class="headerlink" href="#batch-submission-of-yt-visualization-scripts" title="Permalink to this headline">¶</a></h3>
<div class="section" id="rheapreferred-method">
<h4>Rhea—preferred method<a class="headerlink" href="#rheapreferred-method" title="Permalink to this headline">¶</a></h4>
<blockquote>
<div><p>this section needs to be updated. See the titan section</p>
</div></blockquote>
<p>The best way to do visualization is to use rhea, the OLCF vis machine.
You need to build yt via the install_script.sh script <em>on
rhea</em>. It also must be on a <em>Lustre filesystem</em>, so it is seen by
the compute node. It is best to build it in your $PROJWORK directory,
since that has a longer time between purges. Once the installation is
complete, it will tell you what script to source to define the
necessary paths.</p>
<p>The scripts in MAESTRO/Util/job_scripts/rhea/ will handle the
visualization. On rhea, the job script gives you access to the
compute node, and then you can run serial jobs or a parallel job with
mpirun. The process-rhea.run script will request the
resources and run the parallel-yt-rhea script.
parallel-yt-rhea will launch the visualization process (defined
via the variables at the top of the script) on all the plotfiles
found to match the prefix defined in the script. Several serial
jobs are run together, with the script using lock files to keep track
of how many processors are in use. When processors free, the next
file in the list is processed, and so on, until there are no more
plotfiles left to process. If a .png file matching the
plotfile prefix is found, then that plotfile is skipped.</p>
<p>Note: the line in parallel-yt-rhea that sources the yt activate script may need to be modified to point to the
correct yt installation path.</p>
</div>
<div class="section" id="titan">
<h4>Titan<a class="headerlink" href="#titan" title="Permalink to this headline">¶</a></h4>
<p>You can run yt python scripts in the titan batch queues to do your
visualization. You need to install yt and all its dependencies
manually somewhere on a <em>Lustre filesystem</em>—this ensures that the
compute nodes can see it. A good choice is the project space, since
that has a longer purge window. The following procedure will setup
the development version of yt (from source)</p>
<ul>
<li><p>create a directory in your $PROJWORK directory named yt/</p></li>
<li><p>in yt/, down load the yt install script:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">bitbucket</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="n">yt_analysis</span><span class="o">/</span><span class="n">yt</span><span class="o">/</span><span class="n">raw</span><span class="o">/</span><span class="n">yt</span><span class="o">/</span><span class="n">doc</span><span class="o">/</span><span class="n">install_script</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
</li>
<li><p>edit the script to use Conda to get the necessary dependencies
and to build yt from source. This is accomplished by setting the
following variables near the top of the script:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">INST_CONDA</span><span class="o">=</span><span class="mi">1</span>
<span class="n">INST_YT_SOURCE</span><span class="o">=</span><span class="mi">1</span>
</pre></div>
</div>
</li>
<li><p>run the script:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">source</span> <span class="n">install_script</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
</li>
</ul>
<p>When the script is done, you will have a new python installation in a sub-directory
called yt-conda/ and the script will tell you how to modify your path
in your .bashrc</p>
<p><strong>Important: make sure that you are not loading any other
python environments in your .bashrc, e.g., via modules.</strong></p>
<p>To test thing out, start up the python shell, and try doing
import yt. If there are no errors, then you are good.</p>
<p>The python code vol.py and submission script
yt-vis-titan.run in MAESTRO/Util/job_scripts/titan/vis/ show
how to do a simple visualization in the batch queue using yt. Note
that vol.py is executable, and that we run it via aprun to
ensure that it is launched on the compute node.</p>
<p>The scripts vis-titan.run and parallel-yt-new in that
same directorywill manage the yt jobs by calling a
python script for each file that matches a pattern.
Note that the actual visualization command itself is launched by
parallel-yt-new, again using the aprun command. But
aprun can only launch a single job at a time, so this means
we cannot easily do (trivally) parallel visualization on a node. For
this reason, running on rhea is preferred.</p>
</div>
</div>
<div class="section" id="remote-visit-visualization-on-lens">
<h3>Remote VisIt Visualization on Lens<a class="headerlink" href="#remote-visit-visualization-on-lens" title="Permalink to this headline">¶</a></h3>
<p><em>Note: this information may be out-of-date. It is recommended that
yt be used instead.</em></p>
<p>For large data files, visualization with VisIt should be done with
a client running on your local machine interfacing with VisIt running
on the remote machine. For the lens machine at NCCS, the proper setup
is described below.</p>
<p>First, on lens, in your .bashrc, add:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>export PATH=&quot;/sw/analysis-x64/visit/bin/&quot;:$PATH
</pre></div>
</div>
<p>(you would think that you could just add module load visit but this
does not seem to work with VisIt.</p>
<p>On your local machine, launch VisIt. Note: this procedure seems to
work with VisIt 2.4.2, but not VisIt 2.5.0 for some reason.</p>
<ul class="simple">
<li><p>First we setup a new host</p>
<ul>
<li><p>From the menu, select <em>options :math:`rightarrow` host profiles</em></p></li>
<li><p>Create a new host by clicking on the <em>New Host</em> button.</p></li>
<li><p>Enter the <em>Host nickname</em> as lens</p></li>
<li><p>Enter the <em>Remote host name</em> as lens.ccs.ornl.gov</p></li>
<li><p>Enter the <em>Path to Visit installation</em> as /sw/analysis-x64/visit (not sure if this is needed)</p></li>
<li><p>Make sure that your <em>username</em> is correct</p></li>
<li><p>Check <em>Tunnel data connections through SSH</em></p></li>
</ul>
</li>
<li><p>Setup the <em>Launch Profiles</em></p>
<ul>
<li><p>Click on the <em>Launch Profiles</em> tab</p></li>
<li><p>Click on the <em>New Profile</em> button</p></li>
<li><p>Enter the <em>Profile name</em> as parallel</p></li>
<li><p>Click on the <em>Parallel</em> tab</p></li>
<li><p>Check <em>Launch parallel engine</em></p></li>
<li><p>Select the <em>Parallel launch method</em> as qsub/mpirun</p></li>
<li><p>Set the <em>Partition / Pool / Queue</em> to computation</p></li>
<li><p>Change the <em>Default number of processors</em> to 8</p></li>
<li><p>Set the <em>Default number of nodes</em> to 2</p></li>
<li><p>Set the <em>Default Bank / Account</em> to AST006</p></li>
<li><p>Set the <em>Default Time Limit</em> to 00:30:00</p></li>
</ul>
</li>
<li><p>Click on <em>Apply</em> and <em>Post</em></p></li>
<li><p>Save your changes by selecting <em>Options :math:`rightarrow` Save Settings</em></p></li>
</ul>
<p>To do remote visualization, select <em>File :math:`rightarrow` Open</em>.
From the drop down list at the top, select lens. You will be
prompted for your password. After that, you can navigate to the
directory on lens with the data.</p>
<p>To make a movie (output sequence of images):</p>
<ul>
<li><p>save a view in VisIt you like as a session file (File <span class="math notranslate nohighlight">\(\rightarrow\)</span> Save session).</p></li>
<li><p>On lens, create a file called files.visit which lists all
of the files you want to visualize, one per line, with /Header
after the filename. This can be done simply as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ls</span> <span class="o">-</span><span class="mi">1</span> <span class="o">|</span> <span class="n">grep</span> <span class="o">-</span><span class="n">v</span> <span class="n">processed</span> <span class="o">|</span> <span class="n">awk</span> <span class="s1">&#39;{print $1&quot;/Header&quot;}&#39;</span> <span class="o">&gt;</span> <span class="n">files</span><span class="o">.</span><span class="n">visit</span>
</pre></div>
</div>
<p>(note: the processed bit is for when you used the script above to
automatically archive the data).</p>
</li>
<li><p>Edit the session file, searching for the name of the plotfile you
originally visualized, and change it to read files.visit. Make
sure that the path is correct. This may appear multiple times.</p></li>
<li><p>Relaunch VisIt locally and restore the session (File <span class="math notranslate nohighlight">\(\rightarrow\)</span> Restore session). It will render the first image. Then reopen (File <span class="math notranslate nohighlight">\(\rightarrow\)</span> ReOpen file). After this is done, the buttons that allow you to move through the files should become active (black).</p></li>
<li><p>Resave the session file</p></li>
<li><p>To generate the frames, you have 2 options:</p>
<ol class="arabic">
<li><p>File <span class="math notranslate nohighlight">\(\rightarrow\)</span> Save movie. Pick <em>New simple movie</em>,
then set the format to <em>PNG</em> and add this to the output box by
clicking the right arrow, then in the very last screen, select:
Later, tell me the command to run.</p>
<p>VisIt will pop up a box showing the command to run. Trying to
get the currently running session of VisIt to generate the frames
seems problamatic. Note: you will probably want to edit out the
-v x.x.x argument in the commandline to not have it force
to use a specific version.</p>
</li>
<li><p>If the session file successfully does the remote visualization
as desired, you can run the movie via the commandline with something like:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">visit</span> <span class="o">-</span><span class="n">movie</span> <span class="o">-</span><span class="nb">format</span> <span class="n">png</span> <span class="o">-</span><span class="n">geometry</span> <span class="mi">1080</span><span class="n">x1080</span> <span class="o">-</span><span class="n">output</span> <span class="n">subchandra_cutoff3_</span> \
    <span class="o">-</span><span class="n">start</span> <span class="mi">0</span> <span class="o">-</span><span class="n">end</span> <span class="mi">44</span> <span class="o">-</span><span class="n">sessionfile</span> <span class="n">subchandra_radvel</span><span class="o">.</span><span class="n">session</span>
</pre></div>
</div>
</li>
</ol>
</li>
</ul>
</div>
</div>
<div class="section" id="working-at-nersc">
<h2>Working at NERSC<a class="headerlink" href="#working-at-nersc" title="Permalink to this headline">¶</a></h2>
<div class="section" id="edison-compilers">
<h3>edison compilers<a class="headerlink" href="#edison-compilers" title="Permalink to this headline">¶</a></h3>
<p>The default compilers on edison are the Intel compilers, but
PGI and Cray also work well</p>
<ul class="simple">
<li><p>Intel 15.0.1 works well on edison/NERSC with MPI/OpenMP</p></li>
<li><p>Intel 16.0.2 works fine.</p></li>
<li><p>Cray 8.4.x has worked in the past, but it has not been used
at NERSC in a while.</p></li>
</ul>
<p>Note: in order to compile, you will need to ensure that both the
python and python_base modules are loaded (via the
module command).</p>
</div>
<div class="section" id="running-jobs">
<h3>Running Jobs<a class="headerlink" href="#running-jobs" title="Permalink to this headline">¶</a></h3>
<p>edison is configured with 24 cores per node split between two Intel
IvyBridge 12-core processors. Each processor connects to 1/2 of the
node’s memory and is called a NUMA node, so there are 2 NUMA nodes per
edison node. Best performance is seen when running with 6 or 12 threads.</p>
<p>Note: edison switched to SLURM as the batch system. Your job is submitted
using the sbatch command. Options to sbatch are specified at the
top of your submission script with #SBATCH as the prefix. These options
can be found on the sbatch manpage. For instance,</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#SBATCH -N 2</span>
<span class="c1">#SBATCH -J myjob</span>
<span class="c1">#SBATCH -A repo-name</span>
<span class="c1">#SBATCH -p regular</span>
<span class="c1">#SBATCH -t 12:00:00</span>
</pre></div>
</div>
<p>will request 2 nodes (-N), under the account repo-name (-J),
in the regular queue, and for a 12-hour window -t.</p>
<p>If you are using OpenMP, then your script should set OMP_NUM_THREADS, e.g.,</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">OMP_NUM_THREADS</span><span class="o">=</span><span class="mi">12</span>
</pre></div>
</div>
<p>By default, SLURM will change directory into the submission directory. The
job is launched from your script using srun, e.g.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">srun</span> <span class="o">-</span><span class="n">n</span> <span class="mi">48</span> <span class="o">./</span><span class="n">main</span><span class="o">.</span><span class="n">Linux</span><span class="o">.</span><span class="n">Cray</span><span class="o">.</span><span class="n">mpi</span><span class="o">.</span><span class="n">exe</span> <span class="n">inputs_3d</span>
</pre></div>
</div>
<p>to run 48 MPI tasks (across the 2 nodes), or</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">OMP_NUM_THREADS</span><span class="o">=</span><span class="mi">6</span>
<span class="n">srun</span> <span class="o">-</span><span class="n">n</span> <span class="mi">8</span> <span class="o">-</span><span class="n">c</span> <span class="mi">6</span> <span class="o">./</span><span class="n">main</span><span class="o">.</span><span class="n">Linux</span><span class="o">.</span><span class="n">Cray</span><span class="o">.</span><span class="n">mpi</span><span class="o">.</span><span class="n">omp</span><span class="o">.</span><span class="n">exe</span> <span class="n">inputs_3d</span>
</pre></div>
</div>
<p>to use 8 MPI tasks each with 6 threads.</p>
<p>The scripts in Util/job_scripts/edison/ provides some examples.</p>
<p>To chain jobs, such that one queues up after the previous job finished,
use the chainslurm.sh script in that same directory. You can view
the job dependency using:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">squeue</span> <span class="o">-</span><span class="n">l</span> <span class="o">-</span><span class="n">j</span> <span class="n">job</span><span class="o">-</span><span class="nb">id</span>
</pre></div>
</div>
<p>where job-id is the number of the job.</p>
<p>Jobs are submitted with sbatch. A job can be canceled using
scancel, and the status can be checked using squeue -u
<em>username</em>.</p>
</div>
<div class="section" id="automatic-restarting-and-archiving-of-data-1">
<span id="id1"></span><h3>Automatic Restarting and Archiving of Data<a class="headerlink" href="#automatic-restarting-and-archiving-of-data-1" title="Permalink to this headline">¶</a></h3>
<p>The same set of submission scripts described for titan are available
for edison at NERSC in Util/job_scripts/edison/. In particular,
the job submission script will set the restart command line parameters
to restart from the most recent checkpoint file in the output directory.</p>
<p>Note: NERSC does not allow for the process script that archives
to HPSS to run in the main job submission script. Instead, a separate
job needs to be run in the “xfer” queue. The script edison.xfer.slurm
in Util/job_scripts/edison/ shows how this works.</p>
<p>Jobs in the xfer queue start up quickly. The best approach is
to start one as you start your main job (or make it dependent on the
main job). The sample process.xrb script will wait for output
and then archive it as it is produced, using the techniques described
for titan above.</p>
<p>To check the status of a job in the xfer queue, use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">squeue</span> <span class="o">-</span><span class="n">u</span> <span class="n">username</span> <span class="o">-</span><span class="n">M</span> <span class="nb">all</span>
</pre></div>
</div>
</div>
<div class="section" id="batch-visualization-using-yt">
<h3>Batch visualization using yt<a class="headerlink" href="#batch-visualization-using-yt" title="Permalink to this headline">¶</a></h3>
<p>yt can be built using the install_script.sh. It has been
tested using the build of yt from source and dependencies via conda,
by setting:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">INST_CONDA</span><span class="o">=</span><span class="mi">1</span>
<span class="n">INST_YT_SOURCE</span><span class="o">=</span><span class="mi">1</span>
</pre></div>
</div>
<p>in the install_script.sh. Once these are set, run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">source</span> <span class="n">install_script</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
<p>Note: installation was done in the home directory.</p>
<p>This way of building yt installs it’s own python and support
libraries in a directory, yt-conda. <strong>Important:</strong> You need
to make sure that your start-up files (typically .bashrc.ext at
NERSC) don’t module load python or any python libraries, as this
will interfere with the conda installation. The install script will
direct you to add the install location to your path.</p>
<p>The scripts parallel-yt and process-edison.slurm in
Util/job_scripts/edison show how to invoke yt to loop over a
series of plotfiles and do visualization. A number of tasks are run
at once on the node, each operating on a separate file. The
parallel-yt script then calls vol.py to do the volume
rendering with yt. Note: it is important that srun be used to
launch the yt script to ensure that it is run on the compute node.</p>
<p>A simple test-yt.slurm script shows how to just call the
yt python script directly, using one node and 24 threads, again
using srun to execute on the compute node.</p>
<p>If you want to keep up with the development version of yt, then you
can update the source in yt-conda/bin/src/yt-hg, using:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hg</span> <span class="n">pull</span>
<span class="n">hg</span> <span class="n">update</span> <span class="n">yt</span>
</pre></div>
</div>
<p>and then rebuild it via:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">setup</span><span class="o">.</span><span class="n">py</span> <span class="n">develop</span> <span class="o">--</span><span class="n">user</span>
</pre></div>
</div>
</div>
<div class="section" id="using-the-amrpostprocesing-python-plotting-scripts-on-hopper">
<h3>Using the AmrPostprocesing python plotting scripts on hopper<a class="headerlink" href="#using-the-amrpostprocesing-python-plotting-scripts-on-hopper" title="Permalink to this headline">¶</a></h3>
<p>To build the fsnapshot.so library, you need to do:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">module</span> <span class="n">load</span> <span class="n">gcc</span>
</pre></div>
</div>
<p>f2py is already in the path, so the library should then build without issue.</p>
<p>Then edit your .bashrc.ext file to set the PYTHONPATH to
the python_plotfile directory, e.g.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">PYTHONPATH</span><span class="o">=</span><span class="s2">&quot;/global/homes/z/zingale/AmrPostprocessing/python&quot;</span>
</pre></div>
</div>
<p>and set the PATH to that directory,</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">PATH</span><span class="o">=</span><span class="s2">&quot;/global/homes/z/zingale/AmrPostprocessing/python:$PATH&quot;</span>
</pre></div>
</div>
<p>To run the script, you need to do:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">module</span> <span class="n">load</span> <span class="n">matplotlib</span>
<span class="n">module</span> <span class="n">load</span> <span class="n">python</span>
</pre></div>
</div>
</div>
<div class="section" id="remote-visualization-on-hopper">
<h3>Remote visualization on hopper<a class="headerlink" href="#remote-visualization-on-hopper" title="Permalink to this headline">¶</a></h3>
<p>VisIt is already configured to work with hopper. If the host does not appear
in your local version of visit, copy the host_nersc_hopper.xml file
from the .visit/allhosts/ directory under the system’s VisIt install path
to your <span class="math notranslate nohighlight">\(\mathtt{\sim}\)</span>/.visit/hosts/ directory.</p>
</div>
</div>
<div class="section" id="working-at-ncsa-blue-waters">
<h2>Working at NCSA (Blue Waters)<a class="headerlink" href="#working-at-ncsa-blue-waters" title="Permalink to this headline">¶</a></h2>
<div class="section" id="overview">
<h3>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h3>
<p>Blue Waters consists of 22,640 Cray XE6 compute nodes and 4,224
Cray XK7 compute nodes.</p>
<p>Each XE node has two AMD Interlagos model 6276 compute units, each of
which has 16 integer cores (thus, a single node has a total of 32 integer
cores). Two integer cores share a multithreaded, 256-bit wide floating
point unit (FPU). If both integer cores have their own thread, each has access
to 128-bit floating point processing, whereas if only one thread is
assigned the process can access all 256 bits. In one major science
application on Blue Waters it was found that having an OpenMP thread for
each integer core gave the best performance, but when starting a new
application it’s best to experiment. One OpenMP thread per FPU may
be better in some cases.</p>
<p>Each compute unit is divided into two NUMA nodes. Cores in
the same NUMA region share a pool of L3 cache. For the same science
application as before it was found that the best performance was achieved
by assigning an MPI task to each NUMA node. Thus, each physical node
has four MPI tasks.</p>
<p>The XK nodes consist of one AMD Interlagos model 6276 compute unit
and an NVIDIA GK110 “Kepler” GPU accelerator (Tesla K20X). The
GPU is configured with 14 streaming multiprocessor units (SMXs), each
of which has 192 single-precision or 64 double-precision CUDA cores. Thus
there are a total of 2688 SP CUDA cores or 896 DP CUDA cores.</p>
<p>For more details, please see
<a class="reference external" href="https://bluewaters.ncsa.illinois.edu/user-guide">https://bluewaters.ncsa.illinois.edu/user-guide</a></p>
</div>
<div class="section" id="bw-compilers">
<h3>BW Compilers<a class="headerlink" href="#bw-compilers" title="Permalink to this headline">¶</a></h3>
<p>The Cray compilers are the default on blue waters, and version
8.3.3 works well with MAESTRO.</p>
</div>
<div class="section" id="monitoring-allocations-1">
<span id="id2"></span><h3>Monitoring Allocations<a class="headerlink" href="#monitoring-allocations-1" title="Permalink to this headline">¶</a></h3>
<p>The usage command will list the current user’s usage and
usage -P <em>project</em> will
list the usage for all users in a project allocation named “project”.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="faq.html" class="btn btn-neutral float-right" title="Frequently Asked Questions" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="makefiles.html" class="btn btn-neutral float-left" title="MAESTROeX Build System" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, MAESTROeX development tem

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>